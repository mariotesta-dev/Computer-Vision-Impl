{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s6laF2tkQHDp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16_A(nn.Module):\n",
        "  def __init__(self, in_channels = 3, classes = 10):\n",
        "    super().__init__()\n",
        "    self.feature = nn.Sequential(\n",
        "        #conv 1\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        #conv 2\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        #conv 3\n",
        "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        #conv 4\n",
        "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        #conv 5\n",
        "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        #input size is 224x224, I have 5 MaxPool2d which halves the dims -> 224/2^5 -> 224/32 = 7 --> last conv layer will output a 7x7 feature\n",
        "        nn.Linear(in_features=512*7*7, out_features=4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(in_features=4096, out_features=4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(in_features=4096, out_features=classes),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    def init_weights(m):\n",
        "      if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0, std=0.01)  # Initialize weights from a normal distribution\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)  # Initialize biases to zero\n",
        "\n",
        "    self.feature.apply(init_weights)\n",
        "    self.classifier.apply(init_weights)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.feature(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "uhZbtq3oQPyM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "jDDBscwZYVK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 train dataset\n",
        "data_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "cifar10_train_dataset = torchvision.datasets.CIFAR10(root='./data', download=True, train=True, transform=data_transforms)\n",
        "train_dataloader = torch.utils.data.DataLoader(cifar10_train_dataset,\n",
        "                                         batch_size = 32,\n",
        "                                         shuffle=True)\n",
        "\n",
        "# Load AlexNet\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = VGG16_A().to(device)\n",
        "model.train()\n",
        "\n",
        "# Define Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "epoch_nums = 3 # small value, just for a working demonstration\n",
        "# Train the network\n",
        "for epoch in range(epoch_nums):\n",
        "\n",
        "  current_loss = 0.0\n",
        "  for i, data in tqdm(enumerate(train_dataloader, start = 0), unit=\"batch\", total=len(train_dataloader), desc=f\"Epoch {epoch}\"):\n",
        "    # get inputs -> data is a list of [inputs, labels]\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # compute statistics\n",
        "    current_loss += loss.item()\n",
        "    if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print(f'current loss: {current_loss / 100:.3f}')\n",
        "            current_loss = 0.0\n",
        "\n",
        "print(\"Training done! Saving trained model to: './trained_model.pth'\")\n",
        "torch.save(model.state_dict(), './trained_model.pth')\n",
        "print(\"Saved.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkFbW6xYYXB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "tKgxQu7YuATm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 test dataset\n",
        "data_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "cifar10_test_dataset = torchvision.datasets.CIFAR10(root='./data', download=True, train=False, transform=data_transforms)\n",
        "test_dataloader = torch.utils.data.DataLoader(cifar10_test_dataset,\n",
        "                                         batch_size = 128,\n",
        "                                         shuffle=False)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = VGG16_A().to(device)\n",
        "model.load_state_dict(torch.load(\"./trained_model.pth\"))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in tqdm(test_dataloader):\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\nAccuracy of AlexNet on CIFAR10: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "yeqSgC6FuAIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
